{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Project Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Welcome to your new role as a data scientist in one of the world's leading **payment service providers companies**.* \n",
    "\n",
    "*Already on your first day business stakeholders ask you to set up a model to forecast the **credit card fraud** of customers. The local management has the target to decrease the number of fraudulent transactions in 2021 by 25%. Can you help them to fulfill this target? How would you structure such a project?* \n",
    "\n",
    "We will use as an inspiration the MS Team Data Science Process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Team Data Science Process (TDSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the following *data science lifecycle*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T08:36:38.370774Z",
     "start_time": "2020-08-08T08:36:38.365824Z"
    }
   },
   "source": [
    "## Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    We approach **business users** and ask how they currently approach credit card fraud. Do they have a set of fixed rules, which they apply to each customer transaction? Do they have a special app developed for that purpose? We take a close look at how they use the app and how helpful it is for their daily work.\n",
    "\n",
    "\n",
    "*    We approach the **IT department** to get an idea of the data systems. Are data stored within a cloud environment or within a traditional database system? Which data warehouse (DWH) architectures are in place?\n",
    "\n",
    "\n",
    "*    Finally, we approach colleagues - for instance from business intelligence department - who have a detailed **business understanding**. We ask which variables are important to figure out fraudulent transactions. Is it the age and the gender of the customer? Is it the distance between the home country of the customer and the transaction country? Later on during modeling we will find out if these variables are indeed important for fraudulent transactions or not. In many cases business users have their \"feeling\" about a problem, but \"feelings\" can be biased and very often does not reflect the real underlying explanatory features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aquisition and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    Now the IT department has granted us access to the data warehouse (DWH) system. We try to understand the columns stored in the tables, and ask IT **how, when, and how often these entries are stored**. We again approach the business to understand this information also from a business side. \n",
    "\n",
    "\n",
    "*    Here we can already make first **statistical tests** if certain columns in these tables are important to forecast credit card fraud. Perhaps we already get a first idea about the **possible accuracy of the system.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*     **Approach**: First we have to understand which problem we want to answer with using machine-learning methods. Is it: \"How many transactions on a given day are fraudulent?\" Is it: \"What is the probability of the next transaction to be fraudulent.\" **The question we ask is closely connected to the method - regression, classification, clustering - we use.**\n",
    "\n",
    "\n",
    "*     **Success measures**: In a lot of meetings with business partners we decide about the success measure of our project. For instance business could ask for a model which finds 50% more fraudulent transactions than the current, rule-based system in place. Then we have to ask if it is realistic that our machine-learning model can outperform current systems by 50%. Perhaps in a first version of the new model even 10% improvement is already a good start and further improvement can potentially lead an even higher value. \n",
    "\n",
    "    We try to find a good estimate for the **ROI (return on investment)**. How much money could our model potentially save the company in the upcoming years? Based on that value the management will be able to estimate the number of resources (data engineers, system architects, project manager...) can be used within the project. \n",
    "    \n",
    "\n",
    "*     We start to set up the roles within our team and the infrastructure we will use. \n",
    "\n",
    "    If many colleagues work on our project it is very helpful to have a **code versioning tool** at place, like git: https://git-scm.com/. Make yourself familiar with **git**, since it is the state-of-the art versioning system used for data science projects. \n",
    "    \n",
    "\n",
    "*    Coding starts! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the git structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with the lifecycle stages of a data science project the git-structure could look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T08:24:07.172513Z",
     "start_time": "2020-08-08T08:24:07.156513Z"
    }
   },
   "source": [
    "```bash\n",
    "├── etl\n",
    "│   ├── README.md\n",
    "│   ├── notebooks\n",
    "│   ├── docs\n",
    "│   ├── src\n",
    "│   │   ├── main.py\n",
    "│   |   ├── funcs\n",
    "├── dev\n",
    "│   ├── README.md\n",
    "│   ├── notebooks\n",
    "│   ├── docs\n",
    "│   ├── src\n",
    "│   │   ├── main.py\n",
    "│   |   ├── funcs\n",
    "├── val\n",
    "│   ├── README.md\n",
    "│   ├── docs\n",
    "│   ├── src\n",
    "│   │   ├── main.py\n",
    "│   |   ├── funcs\n",
    "├── depl\n",
    "│   ├── README.md\n",
    "│   ├── runscript.bat\n",
    "│   ├── docs\n",
    "│   ├── src\n",
    "│   │   ├── main.py\n",
    "│   |   ├── funcs\n",
    "├── README.md\n",
    "├── package.bat\n",
    "```\n",
    "\n",
    "with \n",
    "\n",
    "*    **etl (extract-transform-load)** : here automation code related to data gathering, data warehouse should be created\n",
    "*    **dev (development)** : here the automated machine-learning model should be created\n",
    "*    **val (validation)** : here the final model should be (in-depth) validated\n",
    "*    **depl (deployment)** : here the final version of the machine-learning model adapted for the IT-infrastructure should be created\n",
    "\n",
    "where \n",
    "\n",
    "*    **README.md** : describes the problem and the aim of each section\n",
    "*    **src** : is the automated source-code (with a main.py script)\n",
    "*    **docs** : are documents, like plots, presentations...\n",
    "*    **.bat (.sh) scripts** : are automation scripts\n",
    "*    **package.bat (.sh)** : creation of the python virtual environment and automated download of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Material: Cookiecutter project template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project based on the <a target=\"_blank\" href=\"https://github.com/drivendata/cookiecutter-data-science\">cookiecutter data science project template</a>. \n",
    "\n",
    "```bash\n",
    "    ├── README.md          <- The top-level README for developers using this project.\n",
    "    ├── data\n",
    "    │   ├── external       <- Data from third party sources.\n",
    "    │   ├── interim        <- Intermediate data that has been transformed.\n",
    "    │   ├── processed      <- The final, canonical data sets for modeling.\n",
    "    │   └── raw            <- The original, immutable data dump.\n",
    "    │\n",
    "    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n",
    "    │\n",
    "    ├── models             <- Trained and serialized models, model predictions, or model summaries\n",
    "    │\n",
    "    ├── notebooks          <- Jupyter notebooks. Naming convention requires no spaces and lower case: file_name.ipynb\n",
    "    │\n",
    "    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "    │\n",
    "    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    │   └── figures        <- Generated graphics and figures to be used in reporting\n",
    "    │\n",
    "    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "    │                         generated with `pip freeze > requirements.txt`\n",
    "    │\n",
    "    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n",
    "    ├── src                <- Source code for use in this project.\n",
    "    │   ├── __init__.py    <- Makes src a Python module\n",
    "    │   │\n",
    "    │   ├── data           <- Scripts to download or generate data\n",
    "    │   │   ├── make_dataset.py\n",
    "    │   │   └── __init__.py\n",
    "    │   │\n",
    "    │   ├── features       <- Scripts to turn raw data into features for modeling\n",
    "    │   │   └── build_features.py\n",
    "    │   │\n",
    "    │   ├── models         <- Scripts to train models and then use trained models to make\n",
    "    │   │   │                 predictions\n",
    "    │   │   ├── predict_model.py\n",
    "    │   │   ├── train_model.py\n",
    "    │   │   └── __init__.py\n",
    "    │   │\n",
    "    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "    │       ├── visualize.py\n",
    "    │       └── __init__.py\n",
    "    │\n",
    "    ├── utils\n",
    "    │   ├── __init__.py    <- Makes src a Python module\n",
    "    │   └── functions.py   <- Methods shared across packages\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
